# Co-location

## CPU Socket

<img src="https://raw.githubusercontent.com/KIDSSCC/MarkDown_image/main/Picture1280X1280.PNG" alt="1280X1280" style="zoom:67%;" />

一个CPU Socket可以视为一个处理器，

- core：物理核，一个CPU中可以有多个核，各个核之间相互独立，可以并行执行，每个核拥有自己的寄存器，L1，L2缓存，同一个CPu中的多个物理核共享一个L3 缓存和内存总线。多个core之间是并行
- thread：逻辑核，在一个core中可以有多个thread，逻辑核之间共享缓存和内存控制，每个逻辑核有自己的寄存器。vCPU也是指逻辑核，逻辑核之间是并发



## Parties

多种交互应用下的QoS感知资源分区方法

### 摘要

背景：现有的多租户架构，一个LC与若干个BE，目前的云应用逐渐从批处理作业变为了LC业务

### 1.介绍

多租户的软件架构策略因为资源争夺问题存在性能损失

对于微小的框架和应用，其延迟时间的要求比大型的应用更为严格

原有的三种方法：

- 禁止LC业务和其他业务共享资源，保证了QoS，损失了性能（Q：是指运行LC业务是不能运行其他业务吗）
- 避免可能发生干扰的应用之间的协同调度，提高了利用率，但限制了可以共同调度的情况
- 使用隔离技术进行资源分区，保证了QoS同时提高了BE的吞吐量，但只允许一个LC

Parties的运行粒度：几百毫秒，检测QoS违规

- 没有先验知识
- 容器，线程固定，缓存分区，频率缩放，内存容量分区，磁盘分区，网络带宽分区

#### 论文工作

1. 描述LC业务对不同资源分配的敏感度，对资源干扰的敏感度
2. 资源可交换性
3. 在LC业务和输入负载的多种组合下测试并与Heracles进行对比，

### 2.相关工作

原有工作的两种策略

- 集群调度器根据给定的应用预测干扰，在运行时进行资源调度或者禁止资源共享，保证了QoS但限制了共享
- 内粒度的资源分配，但需要对体系结构或者应用进行更改

parties相比Heracles额外支持内存容量隔离和磁盘带宽隔离

### 3.特征描述

平台规格 ：

![image-20231015195226004](https://raw.githubusercontent.com/KIDSSCC/MarkDown_image/main/Pictureimage-20231015195226004.png)

- 每个socket中分配了8个核心作为IRQ内核，用于网络中断。
- 8GB内存用于操作系统
- 每个应用一个单独的容器
- 启用了超线程和超频

#### 3.1 实验中使用的 LC应用

1. Memcached：高性能内存对象缓存系统，调整了它的数据集配置，包含3200万项，每个项30B的键和200B的值
2. Xapian：web搜索引擎，根据wiki英文版快照构建成了叶子节点
3. Nginx：http服务器，数据集100万个大小为1KB的html文件
4. Moses：一个翻译系统
5. MongoDB：NoSQL数据库，数据集包含10亿条记录，每条记录10个字段，每个字段100B
6. Sphinx：语音识别系统

#### 最大负载和QoS目标的确定

逐渐提高请求数，来测试服务器的承载能力和延迟的情况。

![image-20231015202348459](https://raw.githubusercontent.com/KIDSSCC/MarkDown_image/main/Pictureimage-20231015202348459.png)

对于每一个应用，逐渐增加输入负载，其尾部延迟也随之增加。（每个应用横轴数值范围不一样）通常在整个横轴的60%到80%的范围内，延迟的增加达到拐点，从拐点之后，延迟就快速增长

Qos设置**为拐点的第99百分位**（不完全理解，似乎是指将0到拐点之间的99百分位作为QoS）

最大负载：拐点处的RPS

![image-20231015203325794](https://raw.githubusercontent.com/KIDSSCC/MarkDown_image/main/Pictureimage-20231015203325794.png)

对于各个应用的更为具体的QoS与负载情况

- 用户空间，内核空间，IO处理的CPU时间占比
- 每千条指令 i_cache miss次数
- 每千条指令LLC miss次数
- 内存
- 内存带宽
- 磁盘带宽
- 网络带宽

#### 3.2 测试策略

使用开环负载生成器

对memcached，使用开环的内部负载生成器，对NGINX和MongoDB，使用开环的wrk2和YCSB，对于Moses，Sphinx，和Xapian，使用Tailbench

采用指数间隔到达时间分布来模拟泊松过程

在三台服务器上实例化足够的客户端避免饱和，保证测量到的延迟是服务器延迟。

每个实验运行5次，每次1分钟

#### 3.3 干扰的研究

将每个LC应用与一系列对系统施加不同影响的microbenchmark组合

![image-20231015211748656](https://raw.githubusercontent.com/KIDSSCC/MarkDown_image/main/Pictureimage-20231015211748656.png)

左侧是测试的各种共享资源以及怎样进行的测试

实验中使用的每个物理核中都有两个逻辑核用于两个超线程。应用本身在8个物理核心上，使用8个线程（对应8个逻辑核）

- 为了测试超线程干扰对应用延迟的影响，将8个计算密集型程序与应用部署在了相同的逻辑核上，此时待测应用会与microbenchmark共享同样的超线程中的资源（一共8个超线程）
- 测试CPU干扰的影响，将8个计算密集程序部署在和待测应用相同的物理核上（每个物理核上两个超线程，一个待测应用，一个microbenchmark）
- 测试电源功率的影响，一个socket中22个物理核，8个用于网络中断，8个用于待测应用，在剩下的6个上跑满病毒（**占用了部分电源**）
- 测试LLC 容量的影响，在与待测应用相同的socket上运行缓存抖动程序。（**相当于ban掉了待测应用在LLC上的缓存数据？**）
- 测试LLC带宽的影响，启动12个缓存抖动程序（**占掉了全部的LLC带宽**）
- 测试内存带宽，启动12个内存波动测试程序（**占掉了部分内存带宽**）
- 测试内存容量，用一个内存冲击程序，占用128GB中的120GB（**占掉了部分内存**）
- 测试硬盘带宽，（**不是很理解**）
- 测试网络带宽的影响，运行iperf3客户端将网络带宽占掉

##### 对于干扰的分析

![image-20231015213706618](https://raw.githubusercontent.com/KIDSSCC/MarkDown_image/main/Pictureimage-20231015213706618.png)

结合之前各个应用的延迟变换情况，应用对自己利用率较高的资源较为敏感。在该资源被占用时，收到的影响较大。

有些应用是因为自己对某个资源利用率高才对该资源敏感

而有些应用是因为其QoS过于严格所以对某些资源敏感（即便其本身可能对该资源没有很高的使用率）

#### 3.4 隔离的研究

两个结论：

1. 每个应用程序都受到一定资源的干扰
2. 每种共享资源都会影响一些程序

采用软硬件隔离机制

区分资源分配敏感型和资源争用敏感型（**是指应用对资源敏感的两种原因吗？**）

两类资源：计算和存储

##### 资源隔离的实验设计

首先单独采用隔离策略运行待测应用，区分出资源分配敏感和资源争用敏感（**为什么会有资源争用敏感？**）

然后再将待测应用和microbenchmark放在一起，探究隔离机制消除干扰的程度

分为计算资源和存储资源两类

具体的隔离机制：

![image-20231015211748656](https://raw.githubusercontent.com/KIDSSCC/MarkDown_image/main/Pictureimage-20231015211748656.png)

可以给应用分配特定的核心，并设定特定核心的频率。Intel CAT可以调整分配的带宽。

Linux上工具可以限制某个容器的 内存容量，磁盘带宽。以上这些是具体的资源隔离方案。即预先分配好一定的资源。在单独运行待测应用的情况下，就可以观察出待测应用是否是因为资源的分配而受到了干扰。

###### 3.4.1对于计算资源

![image-20231016105444491](https://raw.githubusercontent.com/KIDSSCC/MarkDown_image/main/Pictureimage-20231016105444491.png)

在30%和90%负载下，对资源的敏感程度。图中代表了三个维度的资源。横纵轴代表对应的核心与LLC的分配，每个格子中的颜色代表满足QoS所需要的频率（Power）

1. 除MongoDB外，其他应用都需要满足特定的内核数量要求，否则无论LLC怎样分配，都会违反QoS
2. 内核给的够多，LLC和频率都可以放宽要求
3. 低负载下多数应用对LLC分配不敏感。（基本没有说满足特定cache way的需要，cacheway为1也可以满足QoS）

在采用CAT技术隔离cache时，同时也会间接影响内存带宽。因此可能无法区分出应用到底是对cache敏感还是对内存带宽敏感。加了一组实验

![image-20231016110835513](https://raw.githubusercontent.com/KIDSSCC/MarkDown_image/main/Pictureimage-20231016110835513.png)

在空闲的核心上运行了占用内存带宽的microbenchmark，发现应用对LLC的需求进一步提高

###### 3.4.2对于存储资源

仅考虑了MongoDB

![image-20231016111205762](https://raw.githubusercontent.com/KIDSSCC/MarkDown_image/main/Pictureimage-20231016111205762.png)

增大内存容量，对带宽的需求就会相应的减小

###### 3.4.3 资源可替代性

资源的可替代性是的parties可以更快的找到满足要求的配置，原因：

1. 配置方案相对灵活
2. 启发式搜索找到一个解就可以

### 4. parties的设计

#### 4.1 设计原则

1. 动态细粒度的资源分配：细粒度的决策避免在敏感资源上导致QoS违规
2. 不需要先验知识
3. 控制器需要快速的从错误的决策中恢复
4. 最后再考虑工作负载迁移

#### 4.2 控制器

##### 4.2.1 主控制器操作

1. 初始化时所有资源均等分配，每500ms采样一次
2. 当一个应用空闲很小或为负时，为其分配更多的资源
3. 满足所有应用的QoS之后，找到延迟松弛最高的应用减少其资源
4. 设置计时器追踪发生QoS的时间，1min内没有找到解决违规问题就会进行工作负载迁移

应用迁移的原则

应用迁移的步骤

##### 4.2.2 分配的调整

调整一个资源，观察是否满足要求，不满足要求的话就选择调整另一个资源

- 增加资源：目标是减少延迟时间。当增加了某一个资源仍然不满足要求时，不会撤回当前的资源，会继续的追加资源
- 减少资源：目标是仍然保证QoS，当减少资源的操作导致QoS违规时，可以快速的进行恢复，并在30s内禁止再次进行减少资源的操作

##### 4.2.3资源的排序

<增加/减少，核心数量/缓存大小/CPU频率/内存容量/磁盘带宽>，一共10种操作

在没有先验知识的情况下，第一次调整是随机选择资源的，随后的调整按照下图中开始轮转。避免总是增加或者减少同一种资源

![image-20231016202536224](https://raw.githubusercontent.com/KIDSSCC/MarkDown_image/main/Pictureimage-20231016202536224.png)

计算资源发生改变后的结果不能及时观察到，因此在计算轮转完一圈后会检查一下内存的松弛情况。如果内存比较松弛，就再转一圈计算轮

需要跳过的情况：

1. 某一个应用已经拿到最多份额或最小份额的当前资源，会跳过本次调整
2. 对于内存中的服务，不会一直减少对其的内存分配

##### 4.2.4 执行资源分配

当需要给一个应用增加资源时。

1. 先从BE业务处回收，否则再从LC业务回收
2. 如果需要回收的是内存资源，则尽量找内存松弛最大的。否则就找延迟松弛最大的 

#### 4.3 一些讨论

1. 通过监控来分类出哪些应用是内存中的应用。避免其出现内存相关的问题
2. 测量延迟的方法
3. 控制器的参数设置：多长时间调整一次，增加资源和回收资源的阈值，负载迁移的触发时间，资源调整的粒度
4. 增加一个应用的资源可能对另一个应用产生的影响
5. 一个应用不会因为长期的回收资源而一直违背QoS
6. 负载迁移的频率
7. parties与作业调度器

### 5 实验评估

#### 5.1 方法论

在多节点部署

运行LC应用之外，同时运行BE业务，以BE业务的吞吐量作为测试

**实验方案：**

1. 先以恒定负载运行，再考虑日常的负载变化
2. 对于每个应用，负载从10%到100%，以10%为增量，这样对于每个应用就是10种负载，（N个应用，测试空间就是10的N方）
3. 30s预热，60s运行，重复3次
