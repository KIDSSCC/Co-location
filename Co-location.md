# Co-location

## CPU Socket

<img src="https://raw.githubusercontent.com/KIDSSCC/MarkDown_image/main/Picture1280X1280.PNG" alt="1280X1280" style="zoom:67%;" />

一个CPU Socket可以视为一个处理器，

- core：物理核，一个CPU中可以有多个核，各个核之间相互独立，可以并行执行，每个核拥有自己的寄存器，L1，L2缓存，同一个CPu中的多个物理核共享一个L3 缓存和内存总线。多个core之间是并行
- thread：逻辑核，在一个core中可以有多个thread，逻辑核之间共享缓存和内存控制，每个逻辑核有自己的寄存器。vCPU也是指逻辑核，逻辑核之间是并发



## Parties

多种交互应用下的QoS感知资源分区方法

### 摘要

背景：现有的多租户架构，一个LC与若干个BE，目前的云应用逐渐从批处理作业变为了LC业务

### 1.介绍

多租户的软件架构策略因为资源争夺问题存在性能损失

对于微小的框架和应用，其延迟时间的要求比大型的应用更为严格

原有的三种方法：

- 禁止LC业务和其他业务共享资源，保证了QoS，损失了性能（Q：是指运行LC业务是不能运行其他业务吗）
- 避免可能发生干扰的应用之间的协同调度，提高了利用率，但限制了可以共同调度的情况
- 使用隔离技术进行资源分区，保证了QoS同时提高了BE的吞吐量，但只允许一个LC

Parties的运行粒度：几百毫秒，检测QoS违规

- 没有先验知识
- 容器，线程固定，缓存分区，频率缩放，内存容量分区，磁盘分区，网络带宽分区

#### 论文工作

1. 描述LC业务对不同资源分配的敏感度，对资源干扰的敏感度
2. 资源可交换性
3. 在LC业务和输入负载的多种组合下测试并与Heracles进行对比，

### 2.相关工作

原有工作的两种策略

- 集群调度器根据给定的应用预测干扰，在运行时进行资源调度或者禁止资源共享，保证了QoS但限制了共享
- 内粒度的资源分配，但需要对体系结构或者应用进行更改

parties相比Heracles额外支持内存容量隔离和磁盘带宽隔离

### 3.特征描述

平台规格 ：

![image-20231015195226004](https://raw.githubusercontent.com/KIDSSCC/MarkDown_image/main/Pictureimage-20231015195226004.png)

- 每个socket中分配了8个核心作为IRQ内核，用于网络中断。
- 8GB内存用于操作系统
- 每个应用一个单独的容器
- 启用了超线程和超频

#### 3.1 实验中使用的 LC应用

1. Memcached：高性能内存对象缓存系统，调整了它的数据集配置，包含3200万项，每个项30B的键和200B的值
2. Xapian：web搜索引擎，根据wiki英文版快照构建成了叶子节点
3. Nginx：http服务器，数据集100万个大小为1KB的html文件
4. Moses：一个翻译系统
5. MongoDB：NoSQL数据库，数据集包含10亿条记录，每条记录10个字段，每个字段100B
6. Sphinx：语音识别系统

#### 最大负载和QoS目标的确定

逐渐提高请求数，来测试服务器的承载能力和延迟的情况。

![image-20231015202348459](https://raw.githubusercontent.com/KIDSSCC/MarkDown_image/main/Pictureimage-20231015202348459.png)

对于每一个应用，逐渐增加输入负载，其尾部延迟也随之增加。（每个应用横轴数值范围不一样）通常在整个横轴的60%到80%的范围内，延迟的增加达到拐点，从拐点之后，延迟就快速增长

Qos设置**为拐点的第99百分位**（不完全理解，似乎是指将0到拐点之间的99百分位作为QoS）

最大负载：拐点处的RPS

![image-20231015203325794](https://raw.githubusercontent.com/KIDSSCC/MarkDown_image/main/Pictureimage-20231015203325794.png)

对于各个应用的更为具体的QoS与负载情况

- 用户空间，内核空间，IO处理的CPU时间占比
- 每千条指令 i_cache miss次数
- 每千条指令LLC miss次数
- 内存
- 内存带宽
- 磁盘带宽
- 网络带宽

#### 3.2 测试策略

使用开环负载生成器

对memcached，使用开环的内部负载生成器，对NGINX和MongoDB，使用开环的wrk2和YCSB，对于Moses，Sphinx，和Xapian，使用Tailbench

采用指数间隔到达时间分布来模拟泊松过程

在三台服务器上实例化足够的客户端避免饱和，保证测量到的延迟是服务器延迟。

每个实验运行5次，每次1分钟

#### 3.3 干扰的研究

将每个LC应用与一系列对系统施加不同影响的microbenchmark组合

![image-20231015211748656](https://raw.githubusercontent.com/KIDSSCC/MarkDown_image/main/Pictureimage-20231015211748656.png)

左侧是测试的各种共享资源以及怎样进行的测试

实验中使用的每个物理核中都有两个逻辑核用于两个超线程。应用本身在8个物理核心上，使用8个线程（对应8个逻辑核）

- 为了测试超线程干扰对应用延迟的影响，将8个计算密集型程序与应用部署在了相同的逻辑核上，此时待测应用会与microbenchmark共享同样的超线程中的资源（一共8个超线程）
- 测试CPU干扰的影响，将8个计算密集程序部署在和待测应用相同的物理核上（每个物理核上两个超线程，一个待测应用，一个microbenchmark）
- 测试电源功率的影响，一个socket中22个物理核，8个用于网络中断，8个用于待测应用，在剩下的6个上跑满病毒（**占用了部分电源**）
- 测试LLC 容量的影响，在与待测应用相同的socket上运行缓存抖动程序。（**相当于ban掉了待测应用在LLC上的缓存数据？**）
- 测试LLC带宽的影响，启动12个缓存抖动程序（**占掉了全部的LLC带宽**）
- 测试内存带宽，启动12个内存波动测试程序（**占掉了部分内存带宽**）
- 测试内存容量，用一个内存冲击程序，占用128GB中的120GB（**占掉了部分内存**）
- 测试硬盘带宽，（**不是很理解**）
- 测试网络带宽的影响，运行iperf3客户端将网络带宽占掉

##### 对于干扰的分析

![image-20231015213706618](https://raw.githubusercontent.com/KIDSSCC/MarkDown_image/main/Pictureimage-20231015213706618.png)

结合之前各个应用的延迟变换情况，应用对自己利用率较高的资源较为敏感。在该资源被占用时，收到的影响较大。

有些应用是因为自己对某个资源利用率高才对该资源敏感

而有些应用是因为其QoS过于严格所以对某些资源敏感（即便其本身可能对该资源没有很高的使用率）

#### 3.4 隔离的研究

两个结论：

1. 每个应用程序都受到一定资源的干扰
2. 每种共享资源都会影响一些程序

采用软硬件隔离机制

区分资源分配敏感型和资源争用敏感型（**是指应用对资源敏感的两种原因吗？**）

两类资源：计算和存储

##### 资源隔离的实验设计

首先单独采用隔离策略运行待测应用，区分出资源分配敏感和资源争用敏感（**为什么会有资源争用敏感？**）

然后再将待测应用和microbenchmark放在一起，探究隔离机制消除干扰的程度

分为计算资源和存储资源两类

具体的隔离机制：

![image-20231015211748656](https://raw.githubusercontent.com/KIDSSCC/MarkDown_image/main/Pictureimage-20231015211748656.png)

可以给应用分配特定的核心，并设定特定核心的频率。Intel CAT可以调整分配的带宽。

Linux上工具可以限制某个容器的 内存容量，磁盘带宽。以上这些是具体的资源隔离方案。即预先分配好一定的资源。在单独运行待测应用的情况下，就可以观察出待测应用是否是因为资源的分配而受到了干扰。

###### 对于计算资源

